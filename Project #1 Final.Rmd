---
title: 'Car Features and Price - Final Report'
author: "Ava Holland, Noah Hill, and Anna Thomley"
date: "10/31/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. From this information we wanted to explore further and analyzed:

*What car features (ex: horsepower, citympg, highwaympg, symbolling(insurance)) are most influential on the price of the car?
*Is there a correlation between type of car body and horsepower?


# Introduction

We gathered this data from Kaggle.com. It has data of 205 different individual cars from the US market. The variables are:

* Symboling (Insurance Risk Rating) (-3=safe, +3=risky)
* Car company
* Fuel type (gas or diesel)
* Aspiration (turbo or std)
* Number of Doors
* Car Body Type (convertible, hatchback, sedan, hardtop, or wagon)
* Drive Wheel Type (4wd, rwd, or fwd)
* Engine Location (front or rear)
* Wheelbase in inches
* Length of Car in inches
* Width of Car in inches
* Height of Car in inches
* Weight of Car in pounds
* Engine Type (dohc, ohc, ohcv, rotor, I, ohcf, dohcv,)
* Number of Cylinders (# ranges from 2-12)
* Engine Size in cubic centimeters
* Fuel System (1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi)
* Boreratio in mm
* Stroke in mm
* Compression Ratio in mm^3
* Horsepower
* Peak RPM in thousands
* City Miles per Gallon
* Highway Miles per Gallon
* Price of Car in dollars

Note that this data set is for learning purposes only, we cannot draw any inference with real world scenarios. The response variable is the price of the car.  We are interested in determining which variables can best predict the price of the car.

```{r cars}
#Read the excel file
library(readxl)
CarPrice_Assignment <- read_excel("/Users/avaholland/Downloads/Math 327/Project #1/CarPrice_Assignment.xlsx")
```


# Exploratory Analysis

## Check distributions of the predictor variables

```{r fig.height=7, fig.width = 8}
library(ggplot2)
library(tidyr)
ggplot(gather(CarPrice_Assignment [, c("price", "horsepower", "citympg", "highwaympg", "symboling")]), aes(value)) +
  geom_histogram(bins=10) + 
  facet_wrap(~key, scales = 'free_x')
```

The histogram of horsepower is right skewed. All other predictors have reasonable distributions. We will consider a log transformation for horsepower.


## Log Transformation for Horsepower variable

```{r}
sp = ggplot(gather(CarPrice_Assignment [, c("horsepower")]), aes(value)) +
  geom_histogram(bins=10) + 
  facet_wrap(~key, scales = 'free_x')
sp + scale_x_continuous(trans='log2') +
  scale_y_continuous(trans='log2')

```

After performing this transformation, it allowed us to remove skewness and analyze it with a more normal distribution.

## Pairwise Correlations

```{r fig.height=9, fig.width = 9}
plot(CarPrice_Assignment [, c(10:14, 17, 19:26)])
fit2 = lm (price ~ log(horsepower, 10), data = CarPrice_Assignment)
summary(fit2)
```
Price is the most correlated with horsepower, city miles per gallon, and highway miles per gallon. The correlation between price and horsepower is 0.6196.

Our categorical predictor variables are symboling and carbody.


# Model 1

## First-order model with all predictors

```{r}
fit1 = lm (price ~ horsepower + citympg + highwaympg + carbody + symboling, data = CarPrice_Assignment)
x <- (data = CarPrice_Assignment [, c("price", "horsepower", "citympg", "highwaympg", "symboling")])
pair_data <- data.frame(x)
pairs(pair_data)
summary(fit1)
```

```{r}
library(corrplot)
cormat = cor (CarPrice_Assignment [, c(10:14, 17, 19:26)])
corrplot (cormat)
```

* Using all predictor variables, the model explains 72.54% of the variation in price (adjusted ð‘…2).  
* The residual standard error is $4,186 (price).
* The most significant predictors are horsepower and carbodyhatchback.
* The predictor variables highwaympg and citympg show the highest correlation with each other.  This is shown in the corr plot, the color is the darkest when comparing highwaympg and citympg.

## Residual Analysis - First-order model

```{r fig.width=7.5, fig.height=4}
par (mfrow=c(1,2))
plot(fit1)
```

* There is curvature in the residuals.
* There is more constant residual variance in-between the fitted values when x = 15000 and x = 25000.
* There is non-constant variance in the overall plot.
* The Normal Q-Q plot shows more points above the line on the right side which means the residuals are not normally distributed.  It has a longer tail on the right side.

## Box Cox Analysis

```{r}
library(MASS)
boxcox(fit1)
```

The Box-Cox analysis indicates that optimal transformation would have a power below zero.  Since zero is close to the 95% confidence interval for ðœ†, we will use a log transformation.

# First Order Model -Stepwise Regression

## AIC

```{r}
step.all= step (fit1, direction="both")
summary (fit1)
```

The AIC regression didn't remove any predictor variables from the model. The adjusted R^2 value is 72.54% which suggests a high correlation for our model without the removal of any predictors.


#  Second Order Model With Interactions

## Centering and Interaction Effects

```{r}
#centering
mycenter = function (x) x - mean(x)

CarPrice_Assignment$horsepower.c = mycenter (CarPrice_Assignment$horsepower)

CarPrice_Assignment$citympg.c = mycenter (CarPrice_Assignment$citympg)

CarPrice_Assignment$highwaympg.c = mycenter (CarPrice_Assignment$highwaympg)

CarPrice_Assignment$highwarmpg.c = mycenter (CarPrice_Assignment$highwaympg)

#interaction effects
fit7 = lm (log(price) ~ (horsepower.c + citympg.c + highwaympg.c + carbody + symboling)^2, data = CarPrice_Assignment)
summary(fit7)
```

Through the interaction model and by centering, we concluded that the most significant highly correlated variables are:
*citympg.c:highwaympg.c
*citympg.c:symboling
*highwaympg.c:symboling 


# Second Order Model - Stepwise Regression 
## AIC

```{r}
step.all= step (fit7, direction="both")
summary (fit7)
```

Through the stepwise regression of AIC after centering the interaction effects we found there to be the same three significant interactions between the predictor variables.  These include:  
*citympg:highwaympg

*citympg:symboling

*highwaympg:symboling

The residual standard error is 0.214 with an adjusted R^2 value of 0.8196.

# Final Model

## Residual Diagnostics

```{r}
plot(fit7)
```

Looking at the four residual plots there do not seem to be major concerns.
*Residuals vs fitted values plot: shows linearity and constant variance conditions.

*Normal QQ plot: values fall close to the line, showing the residuals have a normal distribution.

*Square root of absolute standardized residuals vs fitted values: There are no obvious outliers because there is no fall between +-3.

*Standardized residuals vs Leverage, with Cook's distance: Cook's distance show no outliers to be greater than 0.5.

## Box Plot of Residuals

```{r}
boxplot(resid(fit7))
```

The box plot of the residuals show one major outlier around 0.8 with a high standardized residual. The distribution is mostly symetric with a median at 0.

## Plot of Response Variables vs. Fitted Values

```{r}
plot (log(price) ~ fit7$fitted.values, data=CarPrice_Assignment)
abline(0,1)
```

The plot of log(price) vs fitted values from our final model show a good fit of the model to the data.

## Variance Inflation Factors

```{r}
library(car)
vif(fit7)
```

There are 5 VIF values that are less than 5, which is not great for the number of variables and interactions it was analyzing.  This shows a poor collinearity assessment. We also found it strange that there were a large numbers that could have been removed by AIC, so we notice this, and may not conclude practicality from our results.

## High Leverage cutoff for fourth residual plot

```{r}
CarPrice_Assignment$leverage = hatvalues (fit7)

(lev.cut = 3 * fit7$rank / (fit7$rank + fit7$df.residual))

#plot (CarPrice_Assignment [, 2, 22, 24:26], pch=ifelse (CarPrice_Assignment$leverage > lev.cut, 2, 1))

```

The leverage cutoff is 0.4536585.

## Cook's Distance

```{r}
plot(fit7,which=5)
abline(v=lev.cut)
```
```{r}
summary(cooks.distance(fit7))
```

There are 6 data points (2.9% of the sample size) with leverage values above the cutoff. We expect about 5%, so we have less outliers than a typical data set. There are also no Cook's distance values above the cutoff of 0.5.

## Scatterplot Matrix

```{r}
plot(CarPrice_Assignment[,c("horsepower","citympg","highwaympg","carbody","symboling","price")],
     col=ifelse(hatvalues(fit7)>lev.cut,2,1))

table(CarPrice_Assignment$carbody)
```

The scatter plot matrix above shows the high leverage values as red points that appear around the edges of the two-dimensional scatter plots. We recognize that middle categories of symboling have high leverage. We also see that car body has high leverage values around the car body values of 1 and 2. We determined that R assigned labels to the following:
*1=convertible

*2=hardtop

*3=hatchback

*4=sedan

*5=wagon

Based on this knowledge, we see that convertibles and hardtop cars have the highest leverage values because there are fewer of these types of cars in our data set.

## Interaction plots of the three most significant values

### citympg:highwaympg

```{r}
library(ggplot2)
library(dplyr)
library(ggpubr)
categorize = function (x) {
  quartiles = summary (x) [c(2, 3, 5)]
  result = rep ("Q1", length (x))
  result [(quartiles[1] < x) & (x <= quartiles [2])] = "Q2"
  result [(quartiles[2] < x) & (x <= quartiles [3])] = "Q3"
  result [quartiles[3] < x] = "Q4"
  return (result)
}
ggplot(data = CarPrice_Assignment, aes(x = citympg, y = log(price), 
                                       col = categorize(highwaympg))) +
  geom_point() +
  geom_smooth(method=lm)
```

The first interaction plot between city mpg and highway mpg shows the most interesting results. To understand this graph, we analyzed that R categorized highway mpg into four categories:
*Q1=lowest 25% of highway mpg values

*Q2=25% to 50% of highway mpg values

*Q3=50% to 75% of highway mpg values

*Q4=highest 25% of highway mpg values

With this information we can conclude that the relationship between log(price) and city mpg is stronger(steeper) for cars with low-mid highway mpg values.

### citympg:symboling

```{r}
library(ggplot2)
library(dplyr)
library(ggpubr)

ggplot(data = CarPrice_Assignment, aes(x = citympg, y = log(price), col = as.factor(symboling))) +
  geom_point() +
  geom_smooth(method=lm)
```

The interaction plot between city mpg and symboling has less statistically significant meaning because the slopes for the categories of symboling (-2 to 3) and city mpg are all very close to similar. 

### highwaympg:symboling

```{r}
library(ggplot2)
library(dplyr)
library(ggpubr)

ggplot(data = CarPrice_Assignment, aes(x = highwaympg, y = log(price), col = as.factor(symboling))) +
  geom_point() +
  geom_smooth(method=lm)
```

The interaction plot between highway mpg and symboling has less statistically significant meaning because the slopes for the categories of symboling (-2 to 3) and highway mpg are all very close to similar. 

## Example Predictions

```{r}
pred1=predict(fit7, interval = "prediction")
CarPrice_Assignment$pred.price = exp (pred1 [,1])
CarPrice_Assignment$pred.lower = exp (pred1 [,2])
CarPrice_Assignment$pred.upper = exp (pred1 [,3])
med2=cbind.data.frame(CarPrice_Assignment,pred1)
View(med2)
```

```{r}
CarPrice_Assignment[c(77,33,4,81,75,50),c("horsepower","citympg","highwaympg","carbody","symboling","price", "pred.price", "pred.lower", "pred.upper")]
```

Each of the six example cars whose predictions are shown above, have prediction intervals that include the observed prices of the car. The low and middle car prices, have predicted prices that are very close to the actual prices of the car. The high car prices, had predicted prices that were farther off the observed price values, but the prediction interval still contained the actual price value.

```{r}
mean(CarPrice_Assignment$pred.lower<=CarPrice_Assignment$price & CarPrice_Assignment$price<=CarPrice_Assignment$pred.upper)
```

Among all of the cars in the data set, 95.12% have prediction intervals that contain the observed price. We would expect 95%, so the prediction intervals in our data are close to almost accurate. 

# Conclusion

The final model has an adjusted R^2 = 0.8196, which means that 81.96% of the variation in log sales price is explained by the model. The residual standard error is 0.214 log dollars. Shown by our residual analysis, this final model is consistent with the conditions for doing linear regression.

Our most meaningful interaction effect was the relationship between city mpg and highway mpg. The relationship between log(price) and city mpg is stronger(steeper) for cars with low-mid highway mpg values.

We wounder if there are other variables including fuel type or engine location that would also directly impact the price of the car?
